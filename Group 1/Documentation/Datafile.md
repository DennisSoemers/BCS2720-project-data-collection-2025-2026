
## TABLE OF METRICS
| **metric**                             	| **meaning**                                                                            	| **units**                 	| **how measured**                                                    	| **how it will be used**                                                                                      	|
|----------------------------------------	|----------------------------------------------------------------------------------------	|---------------------------	|---------------------------------------------------------------------	|--------------------------------------------------------------------------------------------------------------	|
| **run_id**                             	| Unique identifier for each training run.                                               	| N/A (string)              	| Auto-generated by the script at run start (UUID or timestamp-based) 	| Just used to link all data from the same training run and not confusing them                                 	|
| **wallclock_seconds_total**            	| Total elapsed real-world training time.                                                	| Seconds                   	| Computed from start and end times.                                  	| Measure of total training duration. can be used to compare training efficiency                               	|
| **algo_name**                          	| Algorithm used (e.g., PPO, SAC).                                                       	| Categorical (string).     	| Taken directly from ML-Agents config.                               	| for grouping and differentiating runs based on the algorithm used                                            	|
| **env_name**                           	| Which Unity/ML-Agents environment is being trained (ex. Walker, BallBalance).          	| Categorical (string).     	| Read from environment setup.                                        	| Used to separate datasets based on the chosen environment                                                    	|
| **learning_rate**                      	| Step size for gradient updates.                                                        	| Float (e.g., 3e-4)        	| From training config (YAML)                                         	| We can use different learning rates to test which ones made the agent reach good performance in fewer steps. 	|
| **batch_size**                         	| Number of samples per training update.                                                 	| Integer                   	| From training config (YAML)                                         	| to inference how the number of samples effects learning stability                                            	|
| **nn_arch_depth**                      	| Number of hidden layers in the policy/value network.                                   	| Integer                   	| From training config (YAML)                                         	| to check if deeper networks perform better or just slower                                                    	|
| **cpu_cores_logical**                  	| Available logical CPU cores.                                                           	| Count (int).              	| os.cpu_count() or system probe.                                     	| to check if more cores reduce total wall-clock time or if cpu affect training at all                         	|
| **ram_total_gb**                       	| Installed RAM on machine.                                                              	| GB                        	| System probe (psutil or OS)                                         	| to compare results fairly between different computers used for training (ex. machie with 8 ram vs 16 ram)    	|
| **final_perf**                         	| Final performance metric (in our case average episodic reward).                        	| Float (reward units) or % 	| Taken from last evaluation window of training logs                  	| the dependent variable in the models                                                                         	|
| **steps_to_threshold**                 	| Number of environment steps until the agent first reaches a set performance threshold. 	| Steps (int)               	| Detect in training logs when performance â‰¥ threshold                	| to evaluate sample efficiency (fewer steps = better)                                                         	|
| **episodic_reward_mean (time-series)** 	| Rolling average of rewards over episodes                                               	| Reward (float)            	| Logged automatically by ML-Agents; record every N updates           	| to derive features from it like early learning slope or max reward reached                                   	|
| **plateau_reward** 	| mean of mean reward value achieved during baseline configuration at which the agents stopped improving in the last 10 logs                                               	| Reward (float)            	| value added only for the 4 non baseline configuartions           	| to give a threshold for steps_to_threshold to achieve                                   	|